# -*- coding: utf-8 -*-
"""Wine Quality Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zm3Vb4seGzsL3coFcDvSSBQ66aKg2Xx_
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("üç∑ Wine Quality Analysis - Machine Learning Project")
print("=" * 60)

# ====================================================================
# 1. DATA LOADING AND EXPLORATION
# ====================================================================

print("\nüìä STEP 1: Loading and Exploring the Dataset")
print("-" * 40)

# Load the wine dataset from scikit-learn
wine_data = load_wine()
wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)
wine_df['target'] = wine_data.target

# Map target numbers to wine class names
target_names = {0: 'Class_0', 1: 'Class_1', 2: 'Class_2'}
wine_df['wine_class'] = wine_df['target'].map(target_names)

print(f"Dataset Shape: {wine_df.shape}")
print(f"Features: {len(wine_data.feature_names)}")
print(f"Classes: {len(wine_data.target_names)}")
print(f"\nTarget Classes: {wine_data.target_names}")

# Display basic information
print("\nüìã Dataset Info:")
print(wine_df.info())

print("\nüìä First 5 rows:")
print(wine_df.head())

print("\nüìà Statistical Summary:")
print(wine_df.describe().round(2))

# ====================================================================
# 2. DATA ANALYSIS AND VISUALIZATION
# ====================================================================

print("\nüìà STEP 2: Exploratory Data Analysis")
print("-" * 40)

# Class distribution
print("Class Distribution:")
class_counts = wine_df['target'].value_counts().sort_index()
for i, count in enumerate(class_counts):
    print(f"  {wine_data.target_names[i]}: {count} samples ({count/len(wine_df)*100:.1f}%)")

# Check for missing values
print(f"\nMissing Values: {wine_df.isnull().sum().sum()}")

# Feature correlation analysis
print("\nüîç Top 10 Features with Highest Correlation to Target:")
correlations = wine_df.drop(['wine_class'], axis=1).corrwith(wine_df['target']).abs().sort_values(ascending=False)
print(correlations.head(10).round(3))

# ====================================================================
# 3. DATA VISUALIZATION
# ====================================================================

print("\nüìä STEP 3: Data Visualization")
print("-" * 40)

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Class distribution
axes[0,0].pie(class_counts.values, labels=wine_data.target_names, autopct='%1.1f%%', startangle=90)
axes[0,0].set_title('Wine Class Distribution')

# 2. Correlation heatmap (top features)
top_features = correlations.head(8).index.tolist() + ['target']
corr_matrix = wine_df[top_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[0,1])
axes[0,1].set_title('Feature Correlation Matrix')

# 3. Feature distribution by class
feature = correlations.index[0]  # Most correlated feature
for i, class_name in enumerate(wine_data.target_names):
    class_data = wine_df[wine_df['target'] == i][feature]
    axes[1,0].hist(class_data, alpha=0.7, label=class_name, bins=20)
axes[1,0].set_xlabel(feature)
axes[1,0].set_ylabel('Frequency')
axes[1,0].set_title(f'Distribution of {feature} by Wine Class')
axes[1,0].legend()

# 4. PCA visualization
pca_2d = PCA(n_components=2)
X_pca = pca_2d.fit_transform(wine_data.data)
colors = ['red', 'blue', 'green']
for i, class_name in enumerate(wine_data.target_names):
    mask = wine_df['target'] == i
    axes[1,1].scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i], label=class_name, alpha=0.7)
axes[1,1].set_xlabel(f'First Principal Component ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')
axes[1,1].set_ylabel(f'Second Principal Component ({pca_2d.explained_variance_ratio_[1]:.1%} variance)')
axes[1,1].set_title('PCA - Wine Classes in 2D Space')
axes[1,1].legend()

plt.tight_layout()
plt.show()

print(f"‚úÖ Visualizations created successfully!")

# ====================================================================
# 4. DATA PREPROCESSING
# ====================================================================

print("\nüîß STEP 4: Data Preprocessing")
print("-" * 40)

# Prepare features and target
X = wine_data.data
y = wine_data.target

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Data preprocessing completed!")

# ====================================================================
# 5. MACHINE LEARNING MODELS
# ====================================================================

print("\nü§ñ STEP 5: Machine Learning Models")
print("-" * 40)

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Support Vector Machine': SVC(random_state=42)
}

# Train and evaluate models
results = {}

for name, model in models.items():
    print(f"\nüîÑ Training {name}...")

    # Use scaled data for LogReg and SVM, original for Random Forest
    if name in ['Logistic Regression', 'Support Vector Machine']:
        X_train_model = X_train_scaled
        X_test_model = X_test_scaled
    else:
        X_train_model = X_train
        X_test_model = X_test

    # Train the model
    model.fit(X_train_model, y_train)

    # Cross-validation
    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='accuracy')

    # Predictions
    y_pred = model.predict(X_test_model)
    test_accuracy = accuracy_score(y_test, y_pred)

    # Store results
    results[name] = {
        'model': model,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'test_accuracy': test_accuracy,
        'predictions': y_pred
    }

    print(f"  Cross-validation: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})")
    print(f"  Test accuracy: {test_accuracy:.3f}")

# ====================================================================
# 6. MODEL EVALUATION AND COMPARISON
# ====================================================================

print("\nüìä STEP 6: Model Evaluation and Comparison")
print("-" * 40)

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'CV Mean': [results[model]['cv_mean'] for model in results.keys()],
    'CV Std': [results[model]['cv_std'] for model in results.keys()],
    'Test Accuracy': [results[model]['test_accuracy'] for model in results.keys()]
})

print("üìà Model Performance Comparison:")
print(comparison_df.round(4))

# Find best model
best_model_name = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']
best_model = results[best_model_name]['model']
best_predictions = results[best_model_name]['predictions']

print(f"\nüèÜ Best Model: {best_model_name}")
print(f"   Test Accuracy: {results[best_model_name]['test_accuracy']:.4f}")

# Detailed evaluation of best model
print(f"\nüìã Detailed Classification Report for {best_model_name}:")
print(classification_report(y_test, best_predictions, target_names=wine_data.target_names))

# Confusion Matrix
print(f"\nüìä Confusion Matrix for {best_model_name}:")
cm = confusion_matrix(y_test, best_predictions)
print(cm)

# ====================================================================
# 7. HYPERPARAMETER TUNING (for best model)
# ====================================================================

print(f"\nüîß STEP 7: Hyperparameter Tuning for {best_model_name}")
print("-" * 40)

if best_model_name == 'Random Forest':
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    X_tune = X_train
elif best_model_name == 'Logistic Regression':
    param_grid = {
        'C': [0.1, 1, 10],
        'solver': ['lbfgs', 'liblinear']
    }
    X_tune = X_train_scaled
else:  # SVM
    param_grid = {
        'C': [0.1, 1, 10],
        'kernel': ['rbf', 'linear'],
        'gamma': ['scale', 'auto']
    }
    X_tune = X_train_scaled

# Create new model instance for tuning
if best_model_name == 'Random Forest':
    tuning_model = RandomForestClassifier(random_state=42)
elif best_model_name == 'Logistic Regression':
    tuning_model = LogisticRegression(random_state=42, max_iter=1000)
else:
    tuning_model = SVC(random_state=42)

# Grid search
grid_search = GridSearchCV(tuning_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_tune, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# Evaluate tuned model
if best_model_name in ['Logistic Regression', 'Support Vector Machine']:
    tuned_predictions = grid_search.best_estimator_.predict(X_test_scaled)
else:
    tuned_predictions = grid_search.best_estimator_.predict(X_test)

tuned_accuracy = accuracy_score(y_test, tuned_predictions)
print(f"Tuned model test accuracy: {tuned_accuracy:.4f}")

# ====================================================================
# 8. FEATURE IMPORTANCE ANALYSIS
# ====================================================================

print(f"\nüîç STEP 8: Feature Importance Analysis")
print("-" * 40)

if best_model_name == 'Random Forest':
    # Feature importance for Random Forest
    feature_importance = pd.DataFrame({
        'feature': wine_data.feature_names,
        'importance': grid_search.best_estimator_.feature_importances_
    }).sort_values('importance', ascending=False)

    print("Top 10 Most Important Features:")
    print(feature_importance.head(10).round(4))

    # Plot feature importance
    plt.figure(figsize=(10, 8))
    top_features = feature_importance.head(10)
    plt.barh(range(len(top_features)), top_features['importance'])
    plt.yticks(range(len(top_features)), top_features['feature'])
    plt.xlabel('Feature Importance')
    plt.title('Top 10 Feature Importance - Random Forest')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

elif best_model_name == 'Logistic Regression':
    # Feature coefficients for Logistic Regression
    coefficients = np.abs(grid_search.best_estimator_.coef_).mean(axis=0)
    feature_importance = pd.DataFrame({
        'feature': wine_data.feature_names,
        'importance': coefficients
    }).sort_values('importance', ascending=False)

    print("Top 10 Most Important Features (by coefficient magnitude):")
    print(feature_importance.head(10).round(4))

# ====================================================================
# 9. FINAL RESULTS SUMMARY
# ====================================================================

print("\nüéØ FINAL RESULTS SUMMARY")
print("=" * 60)
print(f"Dataset: Wine Classification ({len(wine_data.data)} samples, {len(wine_data.feature_names)} features)")
print(f"Classes: {len(wine_data.target_names)} ({', '.join(wine_data.target_names)})")
print(f"\nBest Model: {best_model_name}")
print(f"Original Accuracy: {results[best_model_name]['test_accuracy']:.4f}")
print(f"Tuned Accuracy: {tuned_accuracy:.4f}")
print(f"Improvement: {tuned_accuracy - results[best_model_name]['test_accuracy']:.4f}")

print(f"\nModel Performance Summary:")
for model_name in results.keys():
    acc = results[model_name]['test_accuracy']
    cv_mean = results[model_name]['cv_mean']
    print(f"  {model_name}: Test={acc:.3f}, CV={cv_mean:.3f}")

print(f"\n‚úÖ Analysis Complete! The {best_model_name} model achieved {tuned_accuracy:.1%} accuracy.")
print("   This model can successfully classify wine types based on chemical properties.")

# Save final model information
final_model_info = {
    'best_model': best_model_name,
    'best_params': grid_search.best_params_,
    'test_accuracy': tuned_accuracy,
    'feature_names': wine_data.feature_names,
    'target_names': wine_data.target_names
}

print(f"\nüíæ Model information saved for future use!")
print("   Use the tuned model for making predictions on new wine samples.")